{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import json\n",
    "from collections import Counter\n",
    "import random\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pickle\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Notwendige Daten aus MSCOCO extrahieren\n",
    "Im Folgenden werden aus dem MSCOCO-Datensatz die Daten extrahiert, die für diese Arbeit notwendig sind.\n",
    "Dazu zählen alle inhaltlichen Annotationen zu den Bildern: \n",
    "1. Things (klar abgrenzbare Objekte im Bild)\n",
    "2. Stuff (nicht eindeutig isolierbares Material im Bild)\n",
    "3. Captions (Gesamtbeschreibung des Bildes).\n",
    "\n",
    "Folgende Ordnerstruktur ist notwendig /mscoco-2017/annotations/ mit folgenden Dateien (cocodataset.org):\n",
    "- captions_train2017.json\n",
    "- captions_val2017.json\n",
    "- instances_train2017.json\n",
    "- instances_val2017.json\n",
    "- stuff_train2017.json\n",
    "- stuff_val2017.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to extract the necessary data from the MSCOCO dataset\n",
    "\n",
    "def extractData(captionsFilePath, thingsFilePath, stuffFilePath) :\n",
    "    # STEP-1: pairing validation images with their corresponding one-sentence captions and storing them in 'data'\n",
    "\n",
    "    with open(captionsFilePath) as f:\n",
    "        annFile = json.load(f)\n",
    "    data = {}\n",
    "    captions_dict = {}\n",
    "    for ann in annFile['annotations']:\n",
    "        imgId = ann['image_id']\n",
    "        if imgId not in captions_dict.keys():\n",
    "            captions_dict[imgId] = [ann['caption']]\n",
    "        else:\n",
    "            captions_dict[imgId].append(ann['caption'])\n",
    "\n",
    "    for img in annFile['images']:\n",
    "        imgId = img['id']\n",
    "        currentContainer = {\n",
    "            \"imgName\" : img['file_name'],\n",
    "            \"height\" : img['height'],\n",
    "            \"width\" : img['width'],\n",
    "            \"things\" : [],\n",
    "            \"stuff\" : [],\n",
    "            \"captions\" : captions_dict[imgId]\n",
    "        }\n",
    "        data[imgId] = currentContainer\n",
    "    \n",
    "    # STEP-2: adding thing classes (things that are shown in the image) to the corresponding image represented in 'data'\n",
    "\n",
    "    with open(thingsFilePath) as f:\n",
    "        annFile = json.load(f)\n",
    "    # creating a dictionary that pairs the categoryId to its corresponding description\n",
    "    class_dict = {category['id']: category['name'] for category in annFile['categories']}\n",
    "    # add each thing-description of an annotation to the corresponding image in valData\n",
    "    for ann in annFile['annotations']:\n",
    "        data[ann['image_id']]['things'].append(class_dict.get(ann['category_id']))\n",
    "\n",
    "    # STEP-3: adding stuff classes (stuff that is shown in the image) to the corresponding image represented in 'data'\n",
    "\n",
    "    with open(stuffFilePath) as f:\n",
    "        annFile = json.load(f)\n",
    "    # creating a dictionary that pairs the categoryId to its corresponding description\n",
    "    class_dict = {category['id']: category['name'] for category in annFile['categories']}\n",
    "    # add each thing-description of an annotation to the corresponding image in valData\n",
    "    for ann in annFile['annotations']:\n",
    "        data[ann['image_id']]['stuff'].append(class_dict.get(ann['category_id']))\n",
    "    \n",
    "    print('Created ' + str(len(data)) + ' image-description pairs.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the data and storing it in a JSON file\n",
    "\n",
    "def useSubDataset(abbreveation):\n",
    "    if abbreveation == 'val':\n",
    "        return ['mscoco-2017/annotations/captions_val2017.json', 'mscoco-2017/annotations/instances_val2017.json', 'mscoco-2017/annotations/stuff_val2017.json']\n",
    "    if abbreveation == 'train':\n",
    "        return ['mscoco-2017/annotations/captions_train2017.json', 'mscoco-2017/annotations/instances_train2017.json', 'mscoco-2017/annotations/stuff_train2017.json']\n",
    "\n",
    "valData = extractData(*useSubDataset('val'))\n",
    "#trainData = extractData(*useSubDataset('train'))\n",
    "data = valData #| trainData\n",
    "\n",
    "# filter data for 640x480 imgs\n",
    "filtered_data = {}\n",
    "for key, value in data.items():\n",
    "    if 'height' in value and 'width' in value:  # Sicherstellen, dass die Schlüssel existieren\n",
    "        if value['height'] == 480 and value['width'] == 640:\n",
    "            filtered_data[key] = value\n",
    "data = filtered_data\n",
    "\n",
    "print('Total dataset contains ' + str(len(data)) + ' image-description pairs.')\n",
    "\n",
    "with open('data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Promptbasis für die Bildgenerierung auswählen\n",
    "Alle stuff- und things-Annotationen sowie die 5 Captions zu jedem Bild liegen nun in der data.json Datei vor.\n",
    "Aus diesen Annotationen wird nun für jedes Bild eine lange und eine kurze Promptbasis ausgewählt aus denen im nächsten Schritt die Prompts für die Bildgenerierung erzeugt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a short and a long prompt base for each image\n",
    "\n",
    "with open('data.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# calculating the average caption length in the whole dataset\n",
    "average_caption_length = 0\n",
    "for key, content in data.items():\n",
    "        for caption in content['captions']:\n",
    "                average_caption_length += len(caption.split())\n",
    "average_caption_length = round(average_caption_length / (len(data) * 5))\n",
    "\n",
    "# choosing the long prompt base for each img based on its proximity to the average_caption_length in the whole dataset\n",
    "def sort_by_proximity(strings, standard_length):\n",
    "    def custom_sort(string):\n",
    "        word_count = len(string.split())\n",
    "        return (abs(word_count - standard_length), word_count)\n",
    "\n",
    "    sorted_strings = sorted(strings, key=custom_sort)\n",
    "\n",
    "    return sorted_strings\n",
    "\n",
    "for key, content in data.items():\n",
    "      data[key]['prompt_base'] = {}\n",
    "      data[key]['prompt_base']['long'] = sort_by_proximity(content['captions'], average_caption_length)[0]\n",
    "\n",
    "# choosing a short prompt base for each img based on its amount of appearances in the img\n",
    "for key, content in data.items():\n",
    "        objclasses = content['things'] + content['stuff']\n",
    "        class_counter = Counter(objclasses)\n",
    "        most_common_classes = class_counter.most_common()\n",
    "        if most_common_classes:\n",
    "              data[key]['prompt_base']['short'] = most_common_classes[0][0]\n",
    "        else:\n",
    "              data[key]['prompt_base']['short'] = random.choice(objclasses)\n",
    "\n",
    "with open('data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generierung der Bilder\n",
    "Im Folgenden werden zu jedem referenzierten MSCOCO-Bild in der data.json Datei zwei Bilder generiert.\n",
    "Dafür wird jeweils die kurze und die lange Promptbasis genutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun muss zunächst der [Fooocus-API Server](https://github.com/mrhan1993/Fooocus-API/) gestartet werden. \n",
    "\n",
    "Wird der Fooocus-API-Server über das HdM Deeplearning Cluster betrieben, gibt es SSH-Probleme bei Serveranfragen mit der Python-Requests Bibliothek. Diese Probleme treten nicht auf, schickt man die Anfragen über einen Browser wie z.B. Chrome. Aus diesem Grund wird im Folgenden ein Workaround mit Selenium-Webdirver für Chrome definiert. Dieser muss nicht ausgeführt werden, wenn das Script lokal auf einem PC ausgeführt wird. Dann können die Anfragen vmtl. mit der Requests Bibliothek getätigt werden.\n",
    "\n",
    "- api_server_adress: die Adresse, unter der der API Server erreichbar ist\n",
    "- credentials: Login-Passwort für den aktuellen Job im IAAI-Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import fooocus_login\n",
    "fooocus_api_server_adress = 'https://cerebro.mi.hdm-stuttgart.de/pm074_42695/proxy/8888/'\n",
    "credentials = fooocus_login\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--disable-software-rasterizer\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.get(fooocus_api_server_adress)\n",
    "\n",
    "input_field = driver.find_element(By.XPATH, \"//input[@class='password'][@type='password']\")\n",
    "input_field.send_keys(credentials)\n",
    "input_field.send_keys(Keys.RETURN)\n",
    "\n",
    "def generate_fooocus_img_via_chrome(prompt, negative_prompt):\n",
    "    generation_params = '''{\n",
    "        prompt: \"''' + prompt + '''\",\n",
    "        //negative_prompt: \"''' + negative_prompt + '''\",\n",
    "        base_model_name: \"juggernautXL_version6Rundiffusion.safetensors\",\n",
    "        style_selections: [],\n",
    "        refiner_model_name: 'Realistic_Vision_V6.0_NV_B1.safetensors',\n",
    "        refiner_switch: 0.4,\n",
    "        aspect_ratios_selection: \"640*480\",\n",
    "        async_process: true\n",
    "    }'''\n",
    "    js_code = \"\"\"\n",
    "    return new Promise((resolve, reject) => {\n",
    "        const generationParams = \"\"\" + generation_params + \"\"\";\n",
    "        const host = '\"\"\" + fooocus_api_server_adress + \"\"\"';\n",
    "\n",
    "        fetch(`${host}/v1/generation/text-to-image`, {\n",
    "            method: 'POST',\n",
    "            headers: {\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            body: JSON.stringify(generationParams)\n",
    "        })\n",
    "        .then(response => {\n",
    "            if (!response.ok) {\n",
    "                throw new Error('Network response was not ok');\n",
    "            }\n",
    "            return response.json();\n",
    "        })\n",
    "        .then(data => resolve(data))\n",
    "        .catch(error => reject(error));\n",
    "    });\n",
    "    \"\"\"\n",
    "    result = driver.execute_script(js_code)\n",
    "    return result\n",
    "\n",
    "def get_img_id_from_fooocus_job(job_id):\n",
    "    js_code = \"\"\"\n",
    "    return (function() {\n",
    "        return new Promise((resolve, reject) => {\n",
    "            const url = new URL('\"\"\" + fooocus_api_server_adress + \"\"\"v1/generation/query-job');\n",
    "            url.search = new URLSearchParams({\n",
    "                job_id: '\"\"\" + job_id + \"\"\"',\n",
    "                require_step_preview: \"false\"\n",
    "            }).toString();\n",
    "\n",
    "            fetch(url, {\n",
    "                method: \"GET\",\n",
    "                headers: {\n",
    "                    \"Accept\": \"application/json\"\n",
    "                }\n",
    "            })\n",
    "            .then(response => {\n",
    "                if (!response.ok) {\n",
    "                    throw new Error(\"Network response was not ok\");\n",
    "                }\n",
    "                return response.json();\n",
    "            })\n",
    "            .then(data => {\n",
    "                resolve(data); \n",
    "            })\n",
    "            .catch(error => {\n",
    "                reject(error); \n",
    "            });\n",
    "        });\n",
    "    })();\"\"\"\n",
    "    \n",
    "    data = driver.execute_script(js_code)\n",
    "    return data\n",
    "\n",
    "def safe_fooocus_img(job, filename):\n",
    "    job_result = get_img_id_from_fooocus_job(job)['job_result']\n",
    "    while job_result == None:\n",
    "        job_result = get_img_id_from_fooocus_job(job)['job_result']\n",
    "        time.sleep(5)\n",
    "    # change filepaths according to your system\n",
    "    !cp /home/stud/p/pm074/Fooocus-API/outputs/files/{job_result[0]['url'][28:]} /home/stud/p/pm074/bachelorarbeit/imgs/mixed/{filename}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ai_model(model):\n",
    "    #select model\n",
    "    if model == 'sdxl':\n",
    "        torch.cuda.empty_cache()\n",
    "        return AutoPipelineForText2Image.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "        ).to(\"cuda\")\n",
    "    elif model == 'fooocus':\n",
    "        return generate_fooocus_img_via_chrome\n",
    "\n",
    "def select_imgs_from_mscoco(amount_or_list):\n",
    "    # img selection from mscoco\n",
    "    with open('data.json') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "    if not type(amount_or_list) == list:\n",
    "        amount = amount_or_list\n",
    "        imgs = list(data.keys())\n",
    "        selected_imgs = imgs[amount[0]:(amount[1] + 1)]\n",
    "\n",
    "        print(str(len(selected_imgs)) + ' imgs selected from MSCOCO: ' + str(selected_imgs))\n",
    "    else:\n",
    "        selected_imgs = amount_or_list\n",
    "    \n",
    "    selected_imgs = {key: data[key] for key in selected_imgs}\n",
    "    return selected_imgs\n",
    "\n",
    "def generate_imgs(amount_prompt_or_mscocolist, model):\n",
    "    if not type(amount_prompt_or_mscocolist) == str:\n",
    "        selected_imgs = select_imgs_from_mscoco(amount_prompt_or_mscocolist)\n",
    "\n",
    "    pipeline_text2image = select_ai_model(model)\n",
    "\n",
    "    # img generation with mscoco base\n",
    "    if not type(amount_prompt_or_mscocolist) == str:\n",
    "        for img_id, img in selected_imgs.items():\n",
    "            short_prompt = 'photo of ' + img['prompt_base']['short']\n",
    "            long_prompt = img['prompt_base']['long']\n",
    "\n",
    "            if model == 'fooocus':\n",
    "                job = pipeline_text2image(short_prompt)['job_id']\n",
    "                current_filepath = str(img_id) + '-short-fooocus.png'\n",
    "                safe_fooocus_img(job, current_filepath)\n",
    "                print(current_filepath + ' done.')\n",
    "\n",
    "                job = pipeline_text2image(long_prompt)['job_id']\n",
    "                current_filepath = str(img_id) + '-long-fooocus.png'\n",
    "                safe_fooocus_img(job, current_filepath)\n",
    "                print(current_filepath + ' done.')\n",
    "            else:\n",
    "                short_prompt_img = pipeline_text2image(prompt=short_prompt, height=864, width=1152).images[0]\n",
    "                short_prompt_img = short_prompt_img.resize((640,480), Image.LANCZOS)\n",
    "                current_filepath = 'imgs/mixed/' + str(img_id) + '-short-' + str(model) + '.jpg'\n",
    "                short_prompt_img.save(current_filepath)\n",
    "                print(current_filepath + ' done.')\n",
    "\n",
    "                long_prompt_img = pipeline_text2image(prompt=long_prompt, height=864, width=1152).images[0]\n",
    "                long_prompt_img = long_prompt_img.resize((640,480), Image.LANCZOS)\n",
    "                current_filepath = 'imgs/mixed/' + str(img_id) + '-long-' + str(model) + '.jpg'\n",
    "                long_prompt_img.save(current_filepath)\n",
    "                print(current_filepath + ' done.')\n",
    "\n",
    "    # img generation with prompt\n",
    "    else:\n",
    "        if model == 'fooocus':\n",
    "            job = pipeline_text2image(amount_prompt_or_mscocolist)['job_id']\n",
    "            safe_fooocus_img(job, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + '-fooocus.png')\n",
    "        else:\n",
    "            img = pipeline_text2image(prompt=amount_prompt_or_mscocolist).images[0] \n",
    "            img.save('imgs/mixed/' + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + '-' + model + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_imgs((0, 1060), 'sdxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_coco_imgs(origin_directory, destination_directory, imgs):\n",
    "    for img in imgs:\n",
    "        ! cp {origin_directory}/{img.zfill(12) + '.jpg'} {destination_directory}\n",
    "img_set = []\n",
    "copy_coco_imgs('mscoco-2017/images/val2017', 'img-dataset/coco/', img_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aufbau der Studiendaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bilder in der Studie sollen ein möglichst breites Spektrum an verschiedenen Bildinhalten darstellen, um möglichst große externe Validität (Generalisierbarkeit) herzustellen.\n",
    "\n",
    "Zunächst wird die Zuordnung Bild-ID – Klasse, für alle Bilder im aktuellen COCO-Subdatensatz (alle 640x480 Bilder) erstellt. Da jeder Klasse mehrere Bilder zugeordnet werden können, wird nun für jede Klasse eine Bild-ID zufällig ausgewählt.\n",
    "Jede Bild-ID ist verbunden mit einem short- und einem long-Prompt.\n",
    "\n",
    "Da COCO 80 Bildklassen beinhaltet entsteht so eine Liste von 80 Bild-IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample_of_all_classes():\n",
    "    with open('data.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # create a dict of all classes represented in the current coco subset (all 640x480 imgs) and their corresponding img-ids\n",
    "    id_and_class = []\n",
    "    for img in data.items():\n",
    "        id_and_class.append((img[0], img[1]['prompt_base']['short']))\n",
    "\n",
    "    elements_by_class = {}\n",
    "    for pair in id_and_class:\n",
    "        if pair[1] in elements_by_class:\n",
    "            elements_by_class[pair[1]].append(pair)\n",
    "        else:\n",
    "            elements_by_class[pair[1]] = []\n",
    "            elements_by_class[pair[1]].append(pair)\n",
    "\n",
    "    # select a random img from every class\n",
    "    id_list = []\n",
    "    for coco_class in elements_by_class.items():\n",
    "        chosen_element = random.choice(coco_class[1])\n",
    "        id_list.append(chosen_element)\n",
    "\n",
    "    print(id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die Studie ebenfalls 80 generierte KI Bilder enthalten soll, kann jedes Bild vollständig unterschiedliche Inhalte haben.\n",
    "Auf Grundlage der Bild IDs werden nun zufällig 20 Fooocus-SP, 20 Fooocus-LP, 20 SDXL-SP und 20 SDXL-LP Bilder ausgewählt.\n",
    "\n",
    "Jedes Bild ist nun eines, das auf Grundlage vollständig unterschiedlicher Prompts generiert wurde.\n",
    "Damit gleicht sich kein Bild inhaltlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of ai generated imgs\n",
    "id_list = select_random_sample_of_all_classes()\n",
    "random.shuffle(id_list)\n",
    "for index, img in enumerate(id_list):\n",
    "    if index in range(0, 40):\n",
    "        if index in range(0, 20):\n",
    "            ! cp img-dataset/fooocus-r/{img[0] + '-short-fooocus.png'} img-dataset/study-imgs/\n",
    "        else:\n",
    "            ! cp img-dataset/fooocus-r/{img[0] + '-long-fooocus.png'} img-dataset/study-imgs/\n",
    "    if index in range(40, 81):\n",
    "        if index in range(40, 60):\n",
    "            ! cp img-dataset/sdxl/{img[0] + '-short-sdxl.jpg'} img-dataset/study-imgs/\n",
    "        else:\n",
    "            ! cp img-dataset/sdxl/{img[0] + '-long-sdxl.jpg'} img-dataset/study-imgs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of coco imgs\n",
    "id_list = select_random_sample_of_all_classes()\n",
    "random.shuffle(id_list)\n",
    "for index, img in enumerate(id_list):\n",
    "    if index % 2 == 0:\n",
    "        ! cp img-dataset/coco/{img[0].zfill(12) + '.jpg'} img-dataset/study/study-imgs-coco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting imgs into study-group-1 imgs and study-group-2 imgs\n",
    "coco_imgs = os.listdir('img-dataset/study/study-imgs-coco/')\n",
    "generated_imgs = os.listdir('img-dataset/study/study-imgs-generated/')\n",
    "\n",
    "def split_imgs_into_2_groups(imgs, origin):\n",
    "    for index, img in enumerate(imgs):\n",
    "        if index % 2 == 0:\n",
    "            ! cp {origin}/{img} img-dataset/study/study-group-1/\n",
    "        else:\n",
    "            ! cp {origin}/{img} img-dataset/study/study-group-2/\n",
    "\n",
    "random.shuffle(coco_imgs)\n",
    "split_imgs_into_2_groups(coco_imgs, 'img-dataset/study/study-imgs-coco')\n",
    "\n",
    "sdxl_short = []\n",
    "sdxl_long = []\n",
    "fooocus_short = []\n",
    "fooocus_long = []\n",
    "\n",
    "for img in generated_imgs:\n",
    "    if 'short-sdxl' in img:\n",
    "        sdxl_short.append(img)\n",
    "    elif 'long-sdxl' in img:\n",
    "        sdxl_long.append(img)\n",
    "    elif 'short-fooocus' in img:\n",
    "        fooocus_short.append(img)\n",
    "    elif 'long-fooocus' in img:\n",
    "        fooocus_long.append(img)\n",
    "\n",
    "[random.shuffle(imgs) for imgs in (sdxl_short, sdxl_long, fooocus_short, fooocus_long)]\n",
    "[split_imgs_into_2_groups(imgs, 'img-dataset/study/study-imgs-generated') for imgs in (sdxl_short, sdxl_long, fooocus_short, fooocus_long)]\n",
    "\n",
    "# resizing every sdxl img with LANCZOS algorithm to match the other imgs size\n",
    "study_groups = ['img-dataset/study/study-group-1/', 'img-dataset/study/study-group-2/']\n",
    "\n",
    "for group in study_groups:\n",
    "    img_name_list = os.listdir(group)\n",
    "    for img_name in img_name_list:\n",
    "        if 'sdxl' in img_name:\n",
    "            img_path = group + img_name\n",
    "            img = Image.open(img_path)\n",
    "            resized_img = img.resize((640, 480), Image.LANCZOS)\n",
    "            resized_img.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random order of the imgs for each poll/ study-group \n",
    "def create_random_order(directory):\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    random.shuffle(files)\n",
    "\n",
    "    total_digits = len(str(len(files)))\n",
    "\n",
    "    # rename files with a leading number\n",
    "    for index, filename in enumerate(files):\n",
    "        new_filename = f\"{str(index + 1).zfill(total_digits)}_{filename}\"\n",
    "        old_file_path = os.path.join(directory, filename)\n",
    "        new_file_path = os.path.join(directory, new_filename)\n",
    "\n",
    "        shutil.move(old_file_path, new_file_path)\n",
    "        print(f\"Renamed '{filename}' to '{new_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_groups = ['img-dataset/study/study-group-1/', 'img-dataset/study/study-group-2/']\n",
    "for group in study_groups:\n",
    "    create_random_order(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting study imgs into areas for the area selection task\n",
    "overlay = Image.open('img-dataset/study/areas.png')\n",
    "\n",
    "for group in ['img-dataset/study/study-group-1/', 'img-dataset/study/study-group-2/']:\n",
    "    files = [f for f in os.listdir(group) if os.path.isfile(os.path.join(group, f))]\n",
    "\n",
    "    for img_name in files:\n",
    "        img = Image.open(group + img_name)\n",
    "        if overlay.mode != img.mode:\n",
    "            overlay = overlay.convert(\"RGB\")\n",
    "            img = img.convert(\"RGB\")\n",
    "        blended_image = Image.blend(overlay, img, .4)\n",
    "        blended_image.save(group + 'img-areas/' + img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testen der Bilder mit dem Detection Model\n",
    "Nun wird jedes Bild vom Detection Modell üperprüft. Das Ergebnis sind Logit-Werte. Ein positiver Wert bedeutet, dass das Bild als KI-generiert erkannt wurde, ein negativer Wert, dass es als echtes Bild identifiziert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all locations of all files to be checked by the model in a csv file\n",
    "def list_files(directory):\n",
    "    file_names = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_names.append(os.path.join(root, file)[5:])\n",
    "    random.shuffle(file_names)\n",
    "    return file_names\n",
    "\n",
    "def write_to_csv(file_names, csv_file):\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['src'])\n",
    "        for file_name in file_names:\n",
    "            writer.writerow([file_name])\n",
    "\n",
    "file_names = list_files('imgs')\n",
    "write_to_csv(file_names, 'img_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all previous model output files\n",
    "!rm -r model_outputs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check every img in the imgs folder for being ai-generated \n",
    "detection_model_script_location = 'detection_model/main.py'\n",
    "detection_model_weights_location = 'detection_model/weights'\n",
    "\n",
    "!python {detection_model_script_location} --data_dir imgs/ --out_dir model_outputs --weights_dir {detection_model_weights_location} --csv_file img_locations.csv\n",
    "\n",
    "# creating detection results csv\n",
    "with open('model_outputs/mixed/mixed.csv', 'r', newline='') as model_outputs, open('detection_results.csv', 'w', newline='') as detection_results:\n",
    "    reader = csv.reader(model_outputs)\n",
    "    writer = csv.writer(detection_results)\n",
    "    writer.writerow(['img', 'probability'])\n",
    "\n",
    "    for index, row in enumerate(reader):\n",
    "        if not index == 0:\n",
    "            writer.writerow([row[0], round(expit(float(row[-1])), 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(df):\n",
    "    df['probability'] = np.where(df['probability'] >= 0.5, 1, 0)\n",
    "\n",
    "    sp_predictions = df[df['img'].str.contains('short')]['probability'].tolist()\n",
    "    sp_true_values = [1] * len(sp_predictions)\n",
    "\n",
    "    lp_predictions = df[df['img'].str.contains('long')]['probability'].tolist()\n",
    "    lp_true_values = [1] * len(lp_predictions)\n",
    "\n",
    "    ai_predictions = df[df['img'].str.contains('long|short')]['probability'].tolist()\n",
    "    ai_true_values = [1] * len(ai_predictions)\n",
    "\n",
    "    real_predictions = df[~df['img'].str.contains('long|short')]['probability'].tolist()\n",
    "\n",
    "    total_predictions = ai_predictions + real_predictions\n",
    "\n",
    "    real_predictions = [1 - x for x in real_predictions]\n",
    "    real_true_values = [1] * len(real_predictions)\n",
    "\n",
    "    total_true_values = ai_true_values + [0] * len(real_predictions)\n",
    "\n",
    "    sdxl_predictions = df[df['img'].str.contains('sdxl')]['probability'].tolist()\n",
    "    sdxl_true_values = [1] * len(sdxl_predictions)\n",
    "\n",
    "    fooocus_predictions = df[df['img'].str.contains('fooocus')]['probability'].tolist()\n",
    "    fooocus_true_values = [1] * len(fooocus_predictions)\n",
    "\n",
    "\n",
    "    f1_sp = f1_score(sp_true_values, sp_predictions)\n",
    "    f1_lp = f1_score(lp_true_values, lp_predictions)\n",
    "    f1_real = f1_score(real_true_values, real_predictions)\n",
    "    f1_sdxl = f1_score(sdxl_true_values, sdxl_predictions)\n",
    "    f1_fooocus = f1_score(fooocus_true_values, fooocus_predictions)\n",
    "\n",
    "    f1_total = f1_score (total_true_values, total_predictions)\n",
    "    f1_ai = f1_score(ai_true_values, ai_predictions)\n",
    "    acc_ai = accuracy_score(ai_true_values, ai_predictions)\n",
    "    return [('f1-total', f1_total), ('f1-ai', f1_ai), ('f1-sp', f1_sp), ('f1-lp', f1_lp), ('acc-ai', acc_ai), ('f1-real', f1_real), ('f1-sdxl', f1_sdxl), ('f1-fooocus', f1_fooocus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('detection_results-samesized_imgs.csv')\n",
    "print(f'Detection model F1-scores for all study imgs (same sized): {calculate_f1_scores(df)}')\n",
    "\n",
    "df = pd.read_csv('detection_results-original_imgs.csv')\n",
    "print(f'Detection model F1-scores for all study imgs (originals): {calculate_f1_scores(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auswertung der Umfrageergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_evaluation(imgs_path, answers_path):\n",
    "    df = pd.read_csv(answers_path)\n",
    "    demographics = df.iloc[:, :7]\n",
    "    answers = df.iloc[:, 7:]\n",
    "\n",
    "    # refactoring demographics data\n",
    "    demographics.columns.values[1] = 'Alter'\n",
    "    demographics.columns.values[2] = 'Geschlecht'\n",
    "    demographics.columns.values[3] = 'Bildungsabschluss'\n",
    "    demographics = demographics.drop(demographics.columns[[0, 6]], axis=1)\n",
    "\n",
    "    # building solutions for the current study group\n",
    "    solutions = []\n",
    "    imgs = os.listdir(imgs_path)\n",
    "    imgs.sort()\n",
    "    for img in imgs:\n",
    "        if os.path.isfile(os.path.join(imgs_path, img)) and ('.jpg' in img or '.png' in img):\n",
    "            if 'long' in img:\n",
    "                prompt = 'long'\n",
    "                generated = 1\n",
    "            elif 'short' in img:\n",
    "                prompt = 'short'\n",
    "                generated = 1\n",
    "            else:\n",
    "                prompt = None\n",
    "                generator = None\n",
    "                generated = 0\n",
    "            if 'sdxl' in img:\n",
    "                generator = 'sdxl'\n",
    "            elif 'fooocus' in img:\n",
    "                generator = 'fooocus'\n",
    "\n",
    "            solutions.append({\n",
    "                'generated': generated, \n",
    "                'prompt': prompt,\n",
    "                'generator': generator\n",
    "            })\n",
    "\n",
    "    # splitting the answers into different categories\n",
    "    classifications = df[[col for col in answers.columns if re.match(re.compile(r'^\\[Q\\d{1,2}\\]$'), col)]]\n",
    "    certainties = df[[col for col in answers.columns if re.match(re.compile(r'^\\[Q\\d{1,2}\\] Wie sicher bist du dir\\?$'), col)]]\n",
    "    area_selections = df[[col for col in answers.columns if re.match(re.compile(r'(^\\[Q\\d{1,2}\\] (Gibt es einen Bildbereich an dem du deine Entscheidung festmachst\\?|Welche Bildbereiche haben deine Entscheidung bestimmt\\?)|^\\[Q\\d{1,2}\\]$)'), col)]]\n",
    "    \n",
    "    # calculating f1-scores for each category (sp, lp, real) for all participants\n",
    "    for index, row in classifications.iterrows():\n",
    "        for column in classifications.columns:\n",
    "            if classifications.at[index, column] == 'KI generiert':\n",
    "                classifications.at[index, column] = 1\n",
    "            elif classifications.at[index, column] == 'echt':\n",
    "                classifications.at[index, column] = 0\n",
    "    \n",
    "    classifications_lists = []\n",
    "    for index, row in classifications.iterrows():\n",
    "        classifications_lists.append(row.tolist())\n",
    "    \n",
    "    certainty_lists = []\n",
    "    for index, row in certainties.iterrows():\n",
    "        certainty_lists.append(row.tolist())\n",
    "    \n",
    "    def calc_f1_scores(classification_list):\n",
    "        lp_predictions = []\n",
    "        lp_true_values = []\n",
    "        sp_predictions = []\n",
    "        sp_true_values = []\n",
    "        ai_predictions = []\n",
    "        ai_true_values = []\n",
    "        real_predictions = []\n",
    "        real_true_values = []\n",
    "        fooocus_predictions = []\n",
    "        fooocus_true_values = []\n",
    "        sdxl_predictions = []\n",
    "        sdxl_true_values = []\n",
    "        for index, solution in enumerate(solutions):\n",
    "            if solution['prompt'] == 'long':\n",
    "                lp_predictions.append(classification_list[index])\n",
    "                lp_true_values.append(solution['generated'])\n",
    "                ai_predictions.append(classification_list[index])\n",
    "                ai_true_values.append(solution['generated'])\n",
    "            elif solution['prompt'] == 'short':\n",
    "                sp_predictions.append(classification_list[index])\n",
    "                sp_true_values.append(solution['generated'])\n",
    "                ai_predictions.append(classification_list[index])\n",
    "                ai_true_values.append(solution['generated'])\n",
    "            elif solution['prompt'] is None:\n",
    "                real_predictions.append(classification_list[index])\n",
    "                real_true_values.append(solution['generated'])\n",
    "            if solution['generator'] == 'fooocus':\n",
    "                fooocus_predictions.append(classification_list[index])\n",
    "                fooocus_true_values.append(solution['generated'])\n",
    "            elif solution['generator'] == 'sdxl':\n",
    "                sdxl_predictions.append(classification_list[index])\n",
    "                sdxl_true_values.append(solution['generated'])\n",
    "\n",
    "        total_true_values = ai_true_values + real_true_values\n",
    "        total_predections = ai_predictions + real_predictions\n",
    "        \n",
    "        # inverse lists of classification results and solutions for real imgs to make f1-score calculation possible\n",
    "        real_true_values = [1 - x for x in real_true_values]\n",
    "        real_predictions = [1 - x for x in real_predictions]\n",
    "\n",
    "        f1_sp = f1_score(sp_true_values, sp_predictions)\n",
    "        f1_lp = f1_score(lp_true_values, lp_predictions)\n",
    "        f1_real = f1_score(real_true_values, real_predictions)\n",
    "        f1_ai = f1_score(ai_true_values, ai_predictions)\n",
    "        f1_fooocus = f1_score(fooocus_true_values, fooocus_predictions)\n",
    "        f1_sdxl = f1_score(sdxl_true_values, sdxl_predictions)\n",
    "        f1_total = f1_score(total_true_values, total_predections)\n",
    "        return {'f1-total': f1_total, 'f1-real': f1_real, 'f1-ai': f1_ai, 'f1-sp': f1_sp, 'f1-lp': f1_lp, 'f1-fooocus': f1_fooocus, 'f1-sdxl': f1_sdxl}\n",
    "    \n",
    "    def calc_accuracy_and_recalls(classification_list):\n",
    "        predictions = []\n",
    "        true_values = []\n",
    "        for index, solution in enumerate(solutions):\n",
    "            predictions.append(classification_list[index])\n",
    "            true_values.append(solution['generated'])\n",
    "        accuracy = accuracy_score(true_values, predictions)\n",
    "        recalls = recall_score(true_values, predictions, average=None)\n",
    "        return accuracy, recalls\n",
    "    \n",
    "    def calc_certainties(certainty_list):\n",
    "        certainty_real = []\n",
    "        certainty_ai = []\n",
    "        certainty_sp = []\n",
    "        certainty_lp = []\n",
    "        for index, solution in enumerate(solutions):\n",
    "            if solution['prompt'] == 'long':\n",
    "                certainty_lp.append(certainty_list[index])\n",
    "                certainty_ai.append(certainty_list[index])\n",
    "            elif solution['prompt'] == 'short':\n",
    "                certainty_sp.append(certainty_list[index])\n",
    "                certainty_ai.append(certainty_list[index])\n",
    "            elif solution['prompt'] is None:\n",
    "                certainty_real.append(certainty_list[index])\n",
    "        return {'certainty-real': pd.Series(certainty_real).mean(), 'certainty-ai': pd.Series(certainty_ai).mean(), 'certainty-sp': pd.Series(certainty_sp).mean(), 'certainty-lp': pd.Series(certainty_lp).mean()}\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'f1-total': [],\n",
    "        'f1-real': [],\n",
    "        'f1-ai': [],\n",
    "        'f1-sp': [],\n",
    "        'f1-lp': [],\n",
    "        'f1-sdxl': [],\n",
    "        'f1-fooocus': [],\n",
    "        'accuracy': [],\n",
    "        'recall-real': [],\n",
    "        'recall-ai': [],\n",
    "        'certainty-real': [],\n",
    "        'certainty-ai': [],\n",
    "        'certainty-sp': [],\n",
    "        'certainty-lp': []\n",
    "    }\n",
    "    for classification_list in classifications_lists:\n",
    "        current_f1s = calc_f1_scores(classification_list)\n",
    "        current_accuracy, recalls = calc_accuracy_and_recalls(classification_list)\n",
    "        metrics['f1-total'].append(current_f1s['f1-total'])\n",
    "        metrics['f1-real'].append(current_f1s['f1-real'])\n",
    "        metrics['f1-ai'].append(current_f1s['f1-ai'])\n",
    "        metrics['f1-sp'].append(current_f1s['f1-sp'])\n",
    "        metrics['f1-lp'].append(current_f1s['f1-lp'])\n",
    "        metrics['f1-fooocus'].append(current_f1s['f1-fooocus'])\n",
    "        metrics['f1-sdxl'].append(current_f1s['f1-sdxl'])\n",
    "        metrics['accuracy'].append(current_accuracy)\n",
    "        metrics['recall-real'].append(recalls[0])\n",
    "        metrics['recall-ai'].append(recalls[1])\n",
    "    \n",
    "    for certainty_list in certainty_lists:\n",
    "        current_certainties = calc_certainties(certainty_list)\n",
    "        metrics['certainty-real'].append(current_certainties['certainty-real'])\n",
    "        metrics['certainty-ai'].append(current_certainties['certainty-ai'])\n",
    "        metrics['certainty-sp'].append(current_certainties['certainty-sp'])\n",
    "        metrics['certainty-lp'].append(current_certainties['certainty-lp'])\n",
    "\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    f1_scores_means = (metrics['f1-total'].mean(), metrics['f1-real'].mean(), metrics['f1-ai'].mean(), metrics['f1-sp'].mean(), metrics['f1-lp'].mean())\n",
    "    print(f1_scores_means)\n",
    "    return metrics, demographics, certainties, area_selections, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_g1, demographics_g1, certainties_g1, area_selections_g1, solutions_g1 = results_evaluation('img-dataset/study/study-group-1/imgs/', 'study-responses/BA Gruppe 1.csv')\n",
    "metrics_g2, demographics_g2, certainties_g2, area_selections_g2, solutions_g2 = results_evaluation('img-dataset/study/study-group-2/imgs/', 'study-responses/BA Gruppe 2.csv')\n",
    "metrics = pd.concat([metrics_g1, metrics_g2])\n",
    "demographics = pd.concat([demographics_g1, demographics_g2])\n",
    "certainties = pd.concat([certainties_g1, certainties_g2])\n",
    "area_selections = pd.concat([area_selections_g1, area_selections_g2])\n",
    "\n",
    "f1s = metrics.drop(columns=['accuracy', 'recall-real', 'recall-ai', 'certainty-real', 'certainty-ai', 'certainty-sp', 'certainty-lp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Demographiedaten auswerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten der Altersgruppen\n",
    "bins = range(0, max(demographics['Alter']) + 5, 5)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(demographics['Alter'], bins=bins, edgecolor='black')\n",
    "plt.title('Wie alt bist du?', fontsize=18)\n",
    "plt.xlabel('Alter', fontsize=16)\n",
    "plt.ylabel('Häufigkeit', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.xticks(bins, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.savefig('statistics/imgs/age_histogram.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten der Geschlechter\n",
    "geschlecht_count = demographics['Geschlecht'].value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "geschlecht_count.plot(kind='bar')\n",
    "plt.title('Welchem Geschlecht fühlst du dich zugehörig?', fontsize=18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Häufigkeit', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=0, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.savefig('statistics/imgs/gender.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten der Bildungsabschlüsse\n",
    "abschluss_count = demographics['Bildungsabschluss'].value_counts()\n",
    "reihenfolge = ['Hauptschulabschluss', 'Realschulabschluss', 'Abitur', 'Bachelor', 'Master', 'Promotion', 'Keiner der oben genannten']\n",
    "ordered_count = pd.Series(index=reihenfolge, data={name: abschluss_count.get(name, 0) for name in reihenfolge})\n",
    "plt.figure(figsize=(10, 5))\n",
    "ordered_count.plot(kind='bar')\n",
    "plt.title('Was ist dein höchster Bildungsabschluss?', fontsize=18)\n",
    "#plt.xlabel('Bildungsabschluss', fontsize=14)\n",
    "plt.ylabel('Häufigkeit', fontsize=16)\n",
    "plt.grid(True)\n",
    "labels = ['Hauptschul-\\nabschluss', 'Realschul-\\nabschluss', 'Abitur', 'Bachelor', 'Master', 'Promotion', 'Keiner der\\ngenannten']\n",
    "plt.xticks(ticks=range(len(reihenfolge)), labels=labels, rotation=0, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.savefig('statistics/imgs/education.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten der Betrachtungserfahrung von KI-Bildern\n",
    "frequency = demographics.iloc[:, 3].value_counts()\n",
    "reihenfolge = ['Nein. Ich habe noch nie KI Bilder bewusst gesehen.', 'Ich habe schon einmal KI Bilder gesehen.', 'Ich sehe KI Bilder hin und wieder.', 'Ich sehe KI Bilder regelmäßig.']\n",
    "frequency = pd.Series(index=reihenfolge, data={name: frequency.get(name, 0) for name in reihenfolge})\n",
    "plt.figure(figsize=(10, 5))\n",
    "frequency.plot(kind='bar')\n",
    "plt.title('Hast du schon KI-generierte Bilder gesehen?', fontsize=18)\n",
    "#plt.xlabel('Antworten', fontsize=14)\n",
    "plt.ylabel('Häufigkeit', fontsize=16)\n",
    "wrapped_answers = [\n",
    "    'Nein. Ich habe\\n noch nie KI Bilder\\n bewusst gesehen.', \n",
    "    'Ich habe schon\\n einmal KI Bilder\\n gesehen.', \n",
    "    'Ich sehe KI Bilder\\nhin und wieder.', \n",
    "    'Ich sehe KI Bilder\\nregelmäßig.'\n",
    "]\n",
    "plt.xticks(range(len(wrapped_answers)), wrapped_answers, rotation=0, ha='center', fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistics/imgs/watch_experience.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten der Generierungserfahrung von KI-Bildern\n",
    "frequency = demographics.iloc[:, 4].value_counts()\n",
    "reihenfolge = ['Nein, noch nie.', 'Ja, schon einmal.', 'Ja, hin und wieder.', 'Ja, regelmäßig.']\n",
    "frequency = pd.Series(index=reihenfolge, data={name: frequency.get(name, 0) for name in reihenfolge})\n",
    "plt.figure(figsize=(10, 5))\n",
    "frequency.plot(kind='bar')\n",
    "plt.title('Hast du schon selbst KI Bilder generiert?', fontsize=18)\n",
    "#plt.xlabel('Antworten')\n",
    "plt.ylabel('Häufigkeit', fontsize=16)\n",
    "wrapped_answers = [\n",
    "    'Nein, noch nie', \n",
    "    'Ja, schon einmal', \n",
    "    'Ja, hin und wieder', \n",
    "    'Ja, regelmäßig'\n",
    "]\n",
    "plt.xticks(range(len(wrapped_answers)), wrapped_answers, rotation=0, ha='center', fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistics/imgs/generation_experience.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Einfluss der Erfahrung auf die Erkennungsleistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frage beantworten \"Hängt die Erkennungsleistung von der Erfahrung ab?\"\n",
    "# experience_type ist der index der Spalte in der demographics Tabelle, die die entsprechende Erfahrung mit KI-Bildern abfragt\n",
    "def f1_scores_per_experience_type(experience_type):\n",
    "    group_1 = {'f1-lp': [], 'f1-sp': [], 'f1-ai': [], 'f1-real': [], 'f1-total': []}\n",
    "    group_2 = {'f1-lp': [], 'f1-sp': [], 'f1-ai': [], 'f1-real': [], 'f1-total': []}\n",
    "    group_3 = {'f1-lp': [], 'f1-sp': [], 'f1-ai': [], 'f1-real': [], 'f1-total': []}\n",
    "    group_4 = {'f1-lp': [], 'f1-sp': [], 'f1-ai': [], 'f1-real': [], 'f1-total': []}\n",
    "\n",
    "    answers = []\n",
    "    if experience_type == 3:\n",
    "        answers = ['Nein. Ich habe noch nie KI Bilder bewusst gesehen.', 'Ich habe schon einmal KI Bilder gesehen.', 'Ich sehe KI Bilder hin und wieder.', 'Ich sehe KI Bilder regelmäßig.']\n",
    "    elif experience_type == 4:\n",
    "        answers = ['Nein, noch nie.', 'Ja, schon einmal.', 'Ja, hin und wieder.', 'Ja, regelmäßig.']\n",
    "\n",
    "    for index, participant in enumerate(demographics.iterrows()):\n",
    "        if participant[1][experience_type] == answers[0]:\n",
    "            group_1['f1-lp'].append(f1s['f1-lp'].iloc[index])\n",
    "            group_1['f1-sp'].append(f1s['f1-sp'].iloc[index])\n",
    "            group_1['f1-ai'].append(f1s['f1-ai'].iloc[index])\n",
    "            group_1['f1-real'].append(f1s['f1-real'].iloc[index])\n",
    "            group_1['f1-total'].append(f1s['f1-total'].iloc[index])\n",
    "        elif participant[1][experience_type] == answers[1]:\n",
    "            group_2['f1-lp'].append(f1s['f1-lp'].iloc[index])\n",
    "            group_2['f1-sp'].append(f1s['f1-sp'].iloc[index])\n",
    "            group_2['f1-ai'].append(f1s['f1-ai'].iloc[index])\n",
    "            group_2['f1-real'].append(f1s['f1-real'].iloc[index])\n",
    "            group_2['f1-total'].append(f1s['f1-total'].iloc[index])\n",
    "        elif participant[1][experience_type] == answers[2]:\n",
    "            group_3['f1-lp'].append(f1s['f1-lp'].iloc[index])\n",
    "            group_3['f1-sp'].append(f1s['f1-sp'].iloc[index])\n",
    "            group_3['f1-ai'].append(f1s['f1-ai'].iloc[index])\n",
    "            group_3['f1-real'].append(f1s['f1-real'].iloc[index])\n",
    "            group_3['f1-total'].append(f1s['f1-total'].iloc[index])\n",
    "        elif participant[1][experience_type] == answers[3]:\n",
    "            group_4['f1-lp'].append(f1s['f1-lp'].iloc[index])\n",
    "            group_4['f1-sp'].append(f1s['f1-sp'].iloc[index])\n",
    "            group_4['f1-ai'].append(f1s['f1-ai'].iloc[index])\n",
    "            group_4['f1-real'].append(f1s['f1-real'].iloc[index])\n",
    "            group_4['f1-total'].append(f1s['f1-total'].iloc[index])\n",
    "\n",
    "    group_1 = pd.DataFrame(group_1)\n",
    "    group_2 = pd.DataFrame(group_2)\n",
    "    group_3 = pd.DataFrame(group_3)\n",
    "    group_4 = pd.DataFrame(group_4)\n",
    "\n",
    "    #print(f\"Gruppe 'Nein. Ich habe noch nie KI Bilder bewusst gesehen.': {group_1['f1-lp'].mean(), group_1['f1-sp'].mean(), group_1['f1-ai'].mean(), group_1['f1-real'].mean()}\")\n",
    "    #print(f\"Gruppe 'Ich habe schon einmal KI Bilder gesehen.': {group_2['f1-lp'].mean(), group_2['f1-sp'].mean(), group_2['f1-ai'].mean(), group_2['f1-real'].mean()}\")\n",
    "    #print(f\"Gruppe 'Ich sehe KI Bilder hin und wieder.': {group_3['f1-lp'].mean(), group_3['f1-sp'].mean(), group_3['f1-ai'].mean(), group_3['f1-real'].mean()}\")\n",
    "    #print(f\"Gruppe 'Ich sehe KI Bilder regelmäßig.': {group_4['f1-lp'].mean(), group_4['f1-sp'].mean(), group_4['f1-ai'].mean(), group_4['f1-real'].mean()}\")\n",
    "\n",
    "    return group_1, group_2, group_3, group_4\n",
    "\n",
    "# custom plots\n",
    "def f1_vs_watch_experience(f1_scores, f1_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, scores in enumerate(f1_scores):\n",
    "        x = [i+1 + np.random.uniform(-0.15, 0.15) for _ in range(len(scores))]\n",
    "        y = scores\n",
    "        plt.scatter(x, y, s=20)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "        plt.plot([i + 1 - 0.2, i + 1 + 0.2], [mean_score, mean_score], color='black', linewidth=1.5)\n",
    "        plt.text(i + 1, mean_score + 0.01, f'$\\\\bar{{x}}$ = {mean_score:.4f}', ha='center', va='bottom', fontsize=10,\n",
    "                 bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.ylabel(f1_type)\n",
    "    plt.title(f'{f1_type}: Hast du schon KI generierte Bilder gesehen?')\n",
    "    plt.xticks(range(1, 5), ['Nein. Ich habe\\nnoch nie KI Bilder\\nbewusst gesehen.\\nn = {}'.format(len(f1_scores[0])),\n",
    "                             'Ich habe schon\\neinmal KI Bilder\\ngesehen.\\nn = {}'.format(len(f1_scores[1])),\n",
    "                             'Ich sehe KI Bilder\\nhin und wieder.\\nn = {}'.format(len(f1_scores[2])),\n",
    "                             'Ich sehe KI Bilder\\nregelmäßig.\\nn = {}'.format(len(f1_scores[3]))])\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'statistics/imgs/watch_experience_vs_{f1_type}.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def f1_vs_generation_experience(f1_scores, f1_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, scores in enumerate(f1_scores):\n",
    "        x = [i+1 + np.random.uniform(-0.15, 0.15) for _ in range(len(scores))]\n",
    "        y = scores\n",
    "        plt.scatter(x, y, s=20)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "        plt.plot([i + 1 - 0.2, i + 1 + 0.2], [mean_score, mean_score], color='black', linewidth=1.5)\n",
    "        plt.text(i + 1, mean_score + 0.01, f'$\\\\bar{{x}}$ = {mean_score:.4f}', ha='center', va='bottom', fontsize=10,\n",
    "                 bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.ylabel(f1_type)\n",
    "    plt.title(f'{f1_type}: Hast du schon selbst KI Bilder generiert?')\n",
    "    plt.xticks(range(1, 5), ['Nein, noch nie.\\nn = {}'.format(len(f1_scores[0])),\n",
    "                             'Ja, schon einmal.\\nn = {}'.format(len(f1_scores[1])),\n",
    "                             'Ja, hin und wieder.\\nn = {}'.format(len(f1_scores[2])),\n",
    "                             'Ja, regelmäßig.\\nn = {}'.format(len(f1_scores[3]))])\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'statistics/imgs/generation_experience_vs_{f1_type}.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots\n",
    "def f1_vs_watch_experience(f1_scores, f1_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(f1_scores, showmeans=True)\n",
    "    plt.ylabel(f1_type.upper(), fontsize=16)\n",
    "    plt.title(f'Hast du schon KI generierte Bilder gesehen?', fontsize=20)\n",
    "    plt.xticks(range(1, 5), ['Nein. Ich habe\\nnoch nie KI Bilder\\nbewusst gesehen.\\nn = {}'.format(len(f1_scores[0])),\n",
    "                             'Ich habe schon\\neinmal KI Bilder\\ngesehen.\\nn = {}'.format(len(f1_scores[1])),\n",
    "                             'Ich sehe KI Bilder\\nhin und wieder.\\nn = {}'.format(len(f1_scores[2])),\n",
    "                             'Ich sehe KI Bilder\\nregelmäßig.\\nn = {}'.format(len(f1_scores[3]))], fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'statistics/imgs/watch_experience_vs_{f1_type}.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def f1_vs_generation_experience(f1_scores, f1_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(f1_scores, showmeans=True)\n",
    "    plt.ylabel(f1_type.upper(), fontsize=16)\n",
    "    plt.title(f'Hast du schon selbst KI Bilder generiert?', fontsize=20)\n",
    "    plt.xticks(range(1, 5), ['Nein, noch nie.\\nn = {}'.format(len(f1_scores[0])),\n",
    "                             'Ja, schon einmal.\\nn = {}'.format(len(f1_scores[1])),\n",
    "                             'Ja, hin und wieder.\\nn = {}'.format(len(f1_scores[2])),\n",
    "                             'Ja, regelmäßig.\\nn = {}'.format(len(f1_scores[3]))], fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'statistics/imgs/generation_experience_vs_{f1_type}.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1, group_2, group_3, group_4 = f1_scores_per_experience_type(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_f1_total = group_4['f1-total'].median()\n",
    "mean_f1_total = group_4['f1-total'].mean()\n",
    "Q1 = group_4['f1-total'].quantile(0.25)\n",
    "Q3 = group_4['f1-total'].quantile(0.75)\n",
    "print(f\"Median: {median_f1_total}\")\n",
    "print(f\"Mittelwert: {mean_f1_total}\")\n",
    "print(f\"Interquartilsintervall [Q1; Q3]: [{Q1}; {Q3}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-scores in Abhängigkeit von der Seherfahrung mit KI-Bildern\n",
    "group_1, group_2, group_3, group_4 = f1_scores_per_experience_type(3)\n",
    "for group in [group_1, group_2, group_3, group_4]:\n",
    "    group['f1-mean'] = group[['f1-ai', 'f1-real']].mean(axis=1)\n",
    "\n",
    "for f1 in group_1.columns:\n",
    "    f1_scores = [group_1[f1], group_2[f1], group_3[f1], group_4[f1]]\n",
    "    f1_vs_watch_experience(f1_scores, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-scores in Abhängigkeit von der Generierungserfahrung mit KI-Bildern\n",
    "group_1, group_2, group_3, group_4 = f1_scores_per_experience_type(4)\n",
    "for group in [group_1, group_2, group_3, group_4]:\n",
    "    group['f1-mean'] = group[['f1-ai', 'f1-real']].mean(axis=1)\n",
    "\n",
    "for f1 in group_1.columns:\n",
    "    f1_scores = [group_1[f1], group_2[f1], group_3[f1], group_4[f1]]\n",
    "    f1_vs_generation_experience(f1_scores, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Einfluss der Promptlänge auf die Erkennungsleistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score-Verteilung je Promptlänge\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([f1s['f1-sp'], f1s['f1-lp']], labels=['f1-sp', 'f1-lp'], showmeans=True, medianprops=dict(color=\"orange\"))\n",
    "plt.title('F1-Scores nach Promptlänge', fontsize=16)\n",
    "plt.ylabel('F1-AI', fontsize=14)\n",
    "plt.ylim(0.38, 1.02) \n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 3), ['SP-Bilder', 'LP-Bilder'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(f'statistics/imgs/f1-vs-prompt.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_sp = f1s['f1-sp']\n",
    "f1_lp = f1s['f1-lp']\n",
    "\n",
    "# Berechnung der Kennzahlen für f1-sp\n",
    "q1_sp = np.percentile(f1_sp, 25)\n",
    "median_sp = np.median(f1_sp)\n",
    "q3_sp = np.percentile(f1_sp, 75)\n",
    "iqr_sp = q3_sp - q1_sp\n",
    "mean_sp = np.mean(f1_sp)\n",
    "\n",
    "# Berechnung der Kennzahlen für f1-lp\n",
    "q1_lp = np.percentile(f1_lp, 25)\n",
    "median_lp = np.median(f1_lp)\n",
    "q3_lp = np.percentile(f1_lp, 75)\n",
    "iqr_lp = q3_lp - q1_lp\n",
    "mean_lp = np.mean(f1_lp)\n",
    "\n",
    "# Ausgabe der Kennzahlen\n",
    "print(\"Kennzahlen für f1-sp:\")\n",
    "print(f\"Interquartilsintervall (Q1 - Q3): {q1_sp:.4f} - {q3_sp:4f}\")\n",
    "print(f\"Mittelwert: {mean_sp:.4f}\")\n",
    "print(f\"Median: {median_sp:.4f}\")\n",
    "print(f\"IQR: {iqr_sp:.4f}\")\n",
    "print(\"\\nKennzahlen für f1-lp:\")\n",
    "print(f\"Interquartilsintervall (Q1 - Q3): {q1_lp:.4f} - {q3_lp:.4f}\")\n",
    "print(f\"Mittelwert: {mean_lp:.4f}\")\n",
    "print(f\"Median: {median_lp:.4f}\")\n",
    "print(f\"IQR: {iqr_lp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data for gaussian distribution\n",
    "diffs = f1s['f1-lp'] - f1s['f1-sp']\n",
    "shapiro_statistic, shapiro_p_value = stats.shapiro(diffs)\n",
    "\n",
    "print(\"Shapiro-Wilk Test Statistic:\", shapiro_statistic)\n",
    "print(\"P-Value:\", shapiro_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon_result = stats.wilcoxon(f1s['f1-lp'], f1s['f1-sp'], alternative='greater')\n",
    "print(\"statistic = {:.2f}, p-value = {:.4e}\".format(wilcoxon_result.statistic, wilcoxon_result.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating effect size\n",
    "diffs = f1s['f1-lp'] - f1s['f1-sp']\n",
    "mean_diff = np.mean(diffs)\n",
    "std_diff = np.std(diffs, ddof=1)\n",
    "d = mean_diff / std_diff\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Einfluss der Entscheidungssicherheit auf die Erkennungsleistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ['real', 'ai', 'sp', 'lp']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(metrics['certainty-' + category], metrics['f1-' + category], 'bo')\n",
    "    plt.title(f'F1-{category.upper()}-Score über Sicherheit', fontsize=20)\n",
    "    plt.xlabel('Mittlere Sicherheit', fontsize=16)\n",
    "    plt.ylabel('F1-' + category.upper(), fontsize=16)\n",
    "    plt.ylim(0.38, 1.02) \n",
    "    plt.xlim(0.8, 5.2) \n",
    "    plt.grid(True)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.savefig(f'statistics/imgs/f1-vs-certainty-mean-{category}.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# falls die Sicherheit als Median je Proband gespeichert ist\n",
    "def f1_score_per_certainty_level(certainty_type):\n",
    "    group_1 = []\n",
    "    group_2 = []\n",
    "    group_3 = []\n",
    "    group_4 = []\n",
    "    group_5 = []\n",
    "\n",
    "    for index, participant in enumerate(metrics.iterrows()):\n",
    "        if participant[1][f'certainty-{certainty_type}'] == 1:\n",
    "            group_1.append(f1s[f'f1-{certainty_type}'].iloc[index])\n",
    "        elif participant[1][f'certainty-{certainty_type}'] == 2:\n",
    "            group_2.append(f1s[f'f1-{certainty_type}'].iloc[index])\n",
    "        elif participant[1][f'certainty-{certainty_type}'] == 3:\n",
    "            group_3.append(f1s[f'f1-{certainty_type}'].iloc[index])\n",
    "        elif participant[1][f'certainty-{certainty_type}'] == 4:\n",
    "            group_4.append(f1s[f'f1-{certainty_type}'].iloc[index])\n",
    "        elif participant[1][f'certainty-{certainty_type}'] == 5:\n",
    "            group_5.append(f1s[f'f1-{certainty_type}'].iloc[index])\n",
    "\n",
    "    group_1 = pd.Series(group_1)\n",
    "    group_2 = pd.Series(group_2)\n",
    "    group_3 = pd.Series(group_3)\n",
    "    group_4 = pd.Series(group_4)\n",
    "    group_5 = pd.Series(group_5)\n",
    "\n",
    "    return group_1, group_2, group_3, group_4, group_5\n",
    "\n",
    "def f1_vs_certainty(f1_scores, f1_type, sample_sizes):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(f1_scores, showmeans=True)\n",
    "    plt.ylabel(f1_type)\n",
    "    plt.title(f'{f1_type}: Wie sicher bist du dir?')\n",
    "    plt.grid(True)\n",
    "    labels = [f'{i}\\n(n={size})' for i, size in zip([\"1\", \"2\", \"3\", \"4\", \"5\"], sample_sizes)]\n",
    "    plt.xticks(ticks=range(1, len(labels) + 1), labels=labels)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'statistics/imgs/f1-vs-certainty-median-{f1_type[3:]}.jpg')\n",
    "    plt.show()\n",
    "\n",
    "for category in ['real', 'ai', 'sp', 'lp']:\n",
    "    groups = f1_score_per_certainty_level(category)\n",
    "    f1_scores = [group for group in groups]\n",
    "    sample_sizes = [len(group) for group in groups] \n",
    "    f1_vs_certainty(f1_scores, f'f1-{category}', sample_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Einfluss des Bildursprungs auf die Erkennungsleistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics.boxplot(column=[\"f1-fooocus\", \"f1-sdxl\", \"f1-real\"], showmeans=True, medianprops=dict(color=\"orange\"), color=\"black\")\n",
    "plt.title(\"F1-Scores nach Bildursprung\", fontsize=16)\n",
    "\n",
    "# Linke y-Achsenbeschriftung\n",
    "plt.ylabel(\"F1-AI\", fontsize=14)\n",
    "\n",
    "plt.xlabel(\"\", fontsize=14)\n",
    "plt.xticks([1, 2, 3], [\"Fooocus\", \"SDXL\", \"COCO\"], fontsize=12)\n",
    "plt.ylim(0.38, 1.02)\n",
    "plt.grid(True)\n",
    "\n",
    "# Vertikale Linie zwischen SDXL und COCO hinzufügen\n",
    "plt.axvline(x=2.5, color='grey', linestyle='--')\n",
    "\n",
    "# Zusätzliche y-Achsenbeschriftung auf der rechten Seite\n",
    "ax = plt.gca()\n",
    "ax2 = ax.twinx()  # Erstellt eine zweite Achse, die die gleiche x-Achse teilt\n",
    "ax2.set_ylabel(\"F1-REAL\", fontsize=14)\n",
    "\n",
    "# y-Achsenlimits synchronisieren\n",
    "ax2.set_ylim(ax.get_ylim())\n",
    "\n",
    "plt.savefig('statistics/imgs/f1-vs-origin.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_fooocus = metrics['f1-fooocus']\n",
    "f1_sdxl = metrics['f1-sdxl']\n",
    "f1_real = metrics['f1-real']\n",
    "\n",
    "# Berechnung der Kennzahlen für f1-fooocus\n",
    "q1_fooocus = np.percentile(f1_fooocus, 25)\n",
    "median_fooocus = np.median(f1_fooocus)\n",
    "q3_fooocus = np.percentile(f1_fooocus, 75)\n",
    "iqr_fooocus = q3_fooocus - q1_fooocus\n",
    "mean_fooocus = np.mean(f1_fooocus)\n",
    "\n",
    "# Berechnung der Kennzahlen für f1-sdxl\n",
    "q1_sdxl = np.percentile(f1_sdxl, 25)\n",
    "median_sdxl = np.median(f1_sdxl)\n",
    "q3_sdxl = np.percentile(f1_sdxl, 75)\n",
    "iqr_sdxl = q3_sdxl - q1_sdxl\n",
    "mean_sdxl = np.mean(f1_sdxl)\n",
    "\n",
    "# Berechnung der Kennzahlen für f1-real\n",
    "q1_real = np.percentile(f1_real, 25)\n",
    "median_real = np.median(f1_real)\n",
    "q3_real = np.percentile(f1_real, 75)\n",
    "iqr_real = q3_real - q1_real\n",
    "mean_real = np.mean(f1_real)\n",
    "\n",
    "# Ausgabe der Kennzahlen\n",
    "print(\"Kennzahlen für f1-fooocus:\")\n",
    "print(f\"Interquartilsintervall (Q1 - Q3): {q1_fooocus:.4f} - {q3_fooocus:.4f}\")\n",
    "print(f\"Mittelwert: {mean_fooocus:.4f}\")\n",
    "print(f\"Median: {median_fooocus:.4f}\")\n",
    "print(f\"IQR: {iqr_fooocus:.4f}\")\n",
    "\n",
    "print(\"\\nKennzahlen für f1-sdxl:\")\n",
    "print(f\"Interquartilsintervall (Q1 - Q3): {q1_sdxl:.4f} - {q3_sdxl:.4f}\")\n",
    "print(f\"Mittelwert: {mean_sdxl:.4f}\")\n",
    "print(f\"Median: {median_sdxl:.4f}\")\n",
    "print(f\"IQR: {iqr_sdxl:.4f}\")\n",
    "\n",
    "print(\"\\nKennzahlen für f1-real:\")\n",
    "print(f\"Interquartilsintervall (Q1 - Q3): {q1_real:.4f} - {q3_real:.4f}\")\n",
    "print(f\"Mittelwert: {mean_real:.4f}\")\n",
    "print(f\"Median: {median_real:.4f}\")\n",
    "print(f\"IQR: {iqr_real:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data for gaussian distribution\n",
    "diffs = f1s['f1-sdxl'] - f1s['f1-fooocus']\n",
    "shapiro_statistic, shapiro_p_value = stats.shapiro(diffs)\n",
    "\n",
    "print(\"Shapiro-Wilk Test Statistic:\", shapiro_statistic)\n",
    "print(\"P-Value:\", shapiro_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon_result = stats.wilcoxon(f1s['f1-sdxl'], f1s['f1-fooocus'], alternative='greater')\n",
    "print(\"statistic = {:.2f}, p-value = {:.4e}\".format(wilcoxon_result.statistic, wilcoxon_result.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating effect size\n",
    "diffs = f1s['f1-sdxl'] - f1s['f1-fooocus']\n",
    "mean_diff = np.mean(diffs)\n",
    "std_diff = np.std(diffs, ddof=1)\n",
    "d = mean_diff / std_diff\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Einfluss der Bildbereiche auf die Entscheidung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_selections_by_img_group_1 = []\n",
    "area_selections_by_img_group_2 = []\n",
    "for i in range(0, len(area_selections.columns), 5):\n",
    "    subset = area_selections.iloc[:, i:i+5]\n",
    "    subset1 = subset.iloc[:100, 0:]\n",
    "    subset2 = subset.iloc[100:, 0:]\n",
    "    area_selections_by_img_group_1.append(subset1)\n",
    "    area_selections_by_img_group_2.append(subset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_1 = \"img-dataset/study/study-group-1/imgs/\"\n",
    "directory_2 = \"img-dataset/study/study-group-2/imgs/\"\n",
    "file_paths_1 = [os.path.join(directory_1, filename) for filename in os.listdir(directory_1) if os.path.isfile(os.path.join(directory_1, filename))]\n",
    "file_paths_1.sort()\n",
    "\n",
    "file_paths_2 = [os.path.join(directory_2, filename) for filename in os.listdir(directory_2) if os.path.isfile(os.path.join(directory_2, filename))]\n",
    "file_paths_2.sort()\n",
    "\n",
    "img_paths_1 = file_paths_1[0:]\n",
    "img_paths_2 = file_paths_2[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Gesamtheatmaps erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_selections_by_img_group_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in area_selections_by_img_group_1:\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].replace({'KI generiert': True, 'echt': False})\n",
    "\n",
    "for df in area_selections_by_img_group_2:\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].replace({'KI generiert': True, 'echt': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_end_until_underscore(input_string):\n",
    "    last_underscore_index = input_string.rfind('_')\n",
    "    if last_underscore_index != -1:\n",
    "        result = input_string[last_underscore_index + 1:]\n",
    "    else:\n",
    "        result = input_string\n",
    "    return result\n",
    "\n",
    "def create_heatmap(df, img_number, image_path, group):\n",
    "    img = mpimg.imread(image_path)\n",
    "\n",
    "    counts_ai = np.zeros((3, 3))\n",
    "    counts_real = np.zeros((3, 3))\n",
    "    \n",
    "    valid_decisions_ai = 0\n",
    "    valid_decisions_real = 0\n",
    "    \n",
    "    df_decisions = df.iloc[:, 2:5]\n",
    "    df_truth_value = df.iloc[:, 0]\n",
    "\n",
    "    for index, row in df_decisions.iterrows():\n",
    "        if not row.isna().all():\n",
    "            if df_truth_value[index] == True:\n",
    "                valid_decisions_ai += 1\n",
    "            else:\n",
    "                valid_decisions_real += 1\n",
    "            for row_idx in range(3):\n",
    "                column_value = row[row_idx]\n",
    "                if pd.isna(column_value):\n",
    "                    continue\n",
    "                if ',' in column_value:\n",
    "                    selections = column_value.split(', ')\n",
    "                else:\n",
    "                    selections = [column_value]\n",
    "                for selection in selections:\n",
    "                    col_idx = ord(selection) - ord('A')\n",
    "                    if df_truth_value[index] == True:\n",
    "                        counts_ai[row_idx, col_idx] += 1\n",
    "                    else:\n",
    "                        counts_real[row_idx, col_idx] += 1\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12.8, 4.8))\n",
    "\n",
    "    ax_ai = axes[0]\n",
    "    ax_ai.imshow(img, extent=[0, 640, 480, 0])\n",
    "    cmap_ai = LinearSegmentedColormap.from_list(\"mycmap_ai\", [\"white\", \"red\"])\n",
    "    if valid_decisions_ai > 0:\n",
    "        cax_ai = ax_ai.imshow(counts_ai, interpolation='nearest', cmap=cmap_ai, extent=[0, 640, 480, 0], alpha=0.5)\n",
    "    else:\n",
    "        cax_ai = ax_ai.imshow(np.zeros_like(counts_ai), interpolation='nearest', cmap=cmap_ai, extent=[0, 640, 480, 0], alpha=0)\n",
    "\n",
    "    cbar_ai = fig.colorbar(cax_ai, ax=ax_ai, fraction=0.046, pad=0.04)\n",
    "    cbar_ai.set_label('Anzahl der Entscheidungen', fontsize=14)\n",
    "\n",
    "    ax_ai.set_title('\"KI-generiert\"', fontsize=16)\n",
    "    ax_ai.set_xticks(np.linspace(640 // 6, 640 - 640 // 6, 3))\n",
    "    ax_ai.set_yticks(np.linspace(480 // 6, 480 - 480 // 6, 3))\n",
    "    ax_ai.set_xticklabels(['A', 'B', 'C'])\n",
    "    ax_ai.set_yticklabels(['1', '2', '3'])\n",
    "    ax_ai.set_xlim(0, 640)\n",
    "    ax_ai.set_ylim(480, 0)\n",
    "    \n",
    "    ax_real = axes[1]\n",
    "    ax_real.imshow(img, extent=[0, 640, 480, 0])\n",
    "    cmap_real = LinearSegmentedColormap.from_list(\"mycmap_real\", [\"white\", \"green\"])\n",
    "    if valid_decisions_real > 0:\n",
    "        cax_real = ax_real.imshow(counts_real, interpolation='nearest', cmap=cmap_real, extent=[0, 640, 480, 0], alpha=0.5)\n",
    "    else:\n",
    "        cax_real = ax_real.imshow(np.zeros_like(counts_real), interpolation='nearest', cmap=cmap_real, extent=[0, 640, 480, 0], alpha=0)\n",
    "\n",
    "    cbar_real = fig.colorbar(cax_real, ax=ax_real, fraction=0.046, pad=0.04)\n",
    "    cbar_real.set_label('Anzahl der Entscheidungen', fontsize=14)\n",
    "\n",
    "    ax_real.set_title('\"echt\"', fontsize=16)\n",
    "    ax_real.set_xticks(np.linspace(640 // 6, 640 - 640 // 6, 3))\n",
    "    ax_real.set_yticks(np.linspace(480 // 6, 480 - 480 // 6, 3))\n",
    "    ax_real.set_xticklabels(['A', 'B', 'C'])\n",
    "    ax_real.set_yticklabels(['1', '2', '3'])\n",
    "    ax_real.set_xlim(0, 640)\n",
    "    ax_real.set_ylim(480, 0)\n",
    "    \n",
    "    ax_ai.text(0.5, -0.1, f'n = {valid_decisions_ai}', ha='center', transform=ax_ai.transAxes, fontsize=12)\n",
    "    ax_real.text(0.5, -0.1, f'n = {valid_decisions_real}', ha='center', transform=ax_real.transAxes, fontsize=12)\n",
    "\n",
    "    plt.suptitle(f'Auswahl der Bildbereiche für {extract_from_end_until_underscore(image_path)}', fontsize=20)\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f'statistics/imgs/heatmaps-{group}/heatmap_{img_number + 1}.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, img in enumerate(area_selections_by_img_group_2):\n",
    "    create_heatmap(img, index, img_paths_1[index], 'g2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 Overlap Probandenheatmaps und Detection-Model-Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmaps(area_selections, img_paths):\n",
    "    heatmaps = []\n",
    "    for df in area_selections:\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].replace({'echt': False, 'KI generiert': True})\n",
    "\n",
    "    for img_number, df in enumerate(area_selections):\n",
    "        image_path = img_paths[img_number]\n",
    "        img = mpimg.imread(image_path)\n",
    "\n",
    "        counts_ai = np.zeros((3, 3))\n",
    "        counts_real = np.zeros((3, 3))\n",
    "        \n",
    "        valid_decisions_ai = 0\n",
    "        valid_decisions_real = 0\n",
    "        \n",
    "        df_decisions = df.iloc[:, 2:5]\n",
    "        df_truth_value = df.iloc[:, 0]\n",
    "\n",
    "        for index, row in df_decisions.iterrows():\n",
    "            if not row.isna().all():\n",
    "                if df_truth_value[index] == True:\n",
    "                    valid_decisions_ai += 1\n",
    "                else:\n",
    "                    valid_decisions_real += 1\n",
    "                for row_idx in range(3):\n",
    "                    column_value = row[row_idx]\n",
    "                    if pd.isna(column_value):\n",
    "                        continue\n",
    "                    if ',' in column_value:\n",
    "                        selections = column_value.split(', ')\n",
    "                    else:\n",
    "                        selections = [column_value]\n",
    "                    for selection in selections:\n",
    "                        col_idx = ord(selection) - ord('A')\n",
    "                        if df_truth_value[index] == True:\n",
    "                            counts_ai[row_idx, col_idx] += 1\n",
    "                        else:\n",
    "                            counts_real[row_idx, col_idx] += 1\n",
    "\n",
    "        heatmaps.append((image_path, counts_ai, counts_real))\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps_g1 = get_heatmaps(area_selections_by_img_group_1, img_paths_1)\n",
    "heatmaps_g2 = get_heatmaps(area_selections_by_img_group_2, img_paths_2)\n",
    "with open('./detection_model/model_heatmaps_g1.pkl', 'rb') as f:\n",
    "    model_heatmaps_g1 = pickle.load(f)\n",
    "with open('./detection_model/model_heatmaps_g2.pkl', 'rb') as f:\n",
    "    model_heatmaps_g2 = pickle.load(f)\n",
    "\n",
    "study_heatmaps = heatmaps_g1 + heatmaps_g2\n",
    "model_heatmaps = model_heatmaps_g1 + model_heatmaps_g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_ai = []\n",
    "p_values_ai = []\n",
    "correlations_real = []\n",
    "p_values_real = []\n",
    "for study_heatmap, model_heatmap in zip(study_heatmaps, model_heatmaps):\n",
    "    correlation_ai, p_value_ai = spearmanr(study_heatmap[1].flatten(), abs(model_heatmap[1].flatten()))\n",
    "    correlation_real, p_value_real = spearmanr(study_heatmap[2].flatten(), abs(model_heatmap[2].flatten()))\n",
    "    correlations_ai.append(correlation_ai)\n",
    "    correlations_real.append(correlation_real)\n",
    "    p_values_ai.append(p_value_ai)\n",
    "    p_values_real.append(p_value_real)\n",
    "\n",
    "correlations_ai = np.array(correlations_ai)\n",
    "p_values_ai = np.array(p_values_ai)\n",
    "correlations_real = np.array(correlations_real)\n",
    "p_values_real = np.array(p_values_real)\n",
    "\n",
    "mean_correlation_ai = np.mean(correlations_ai[~np.isnan(correlations_ai)])\n",
    "mean_correlation_real = np.mean(correlations_real[~np.isnan(correlations_real)])\n",
    "mean_p_value_ai = np.mean(p_values_ai[~np.isnan(p_values_ai)])\n",
    "mean_p_value_real = np.mean(p_values_real[~np.isnan(p_values_real)])\n",
    "\n",
    "print(mean_correlation_ai, mean_p_value_ai)\n",
    "print(mean_correlation_real, mean_p_value_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Heatmaps nach Confusion-Matrix-Kategorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifying_categories(solutions, area_selections):\n",
    "    positives_ai = []\n",
    "    positives_real = []\n",
    "    for index, img in enumerate(solutions):\n",
    "        if img['generated'] == 1:\n",
    "            positives_ai.append(area_selections[index])\n",
    "        else:\n",
    "            positives_real.append(area_selections[index])\n",
    "\n",
    "    true_positives_ai = []\n",
    "    false_negatives_ai = []\n",
    "    true_positives_real = []\n",
    "    false_negatives_real = []\n",
    "\n",
    "    for df in positives_ai:\n",
    "        true_positives_ai.append(df[df.iloc[:, 0] == 'KI generiert'])\n",
    "        false_negatives_ai.append(df[df.iloc[:, 0] == 'echt'])\n",
    "    for df in positives_real:\n",
    "        true_positives_real.append(df[df.iloc[:, 0] == 'echt'])\n",
    "        false_negatives_real.append(df[df.iloc[:, 0] == 'KI generiert'])\n",
    "    return positives_ai, positives_real, true_positives_ai, false_negatives_ai, true_positives_real, false_negatives_real\n",
    "\n",
    "positives_ai_g1, positives_real_g1, true_positives_ai_g1, false_negatives_ai_g1, true_positives_real_g1, false_negatives_real_g1 = classifying_categories(solutions_g1, area_selections_by_img_group_1)\n",
    "positives_ai_g2, positives_real_g2, true_positives_ai_g2, false_negatives_ai_g2, true_positives_real_g2, false_negatives_real_g2 = classifying_categories(solutions_g2, area_selections_by_img_group_2)\n",
    "# wie viele Entscheidungen wurden verortet?\n",
    "decisions = 0\n",
    "spatial_decisions = 0\n",
    "general_decision = 0\n",
    "unsure_decisions = 0\n",
    "wrong_labelled_decisions = 0\n",
    "for group in [true_positives_ai_g1, true_positives_ai_g2]:\n",
    "    for img in group:\n",
    "        for row in img.iterrows():\n",
    "            decisions += 1\n",
    "            if row[1][1] == 'Nein, es war ein Gesamteindruck.':\n",
    "                general_decision += 1\n",
    "            if row[1][1] == 'Ich bin mir nicht sicher.':\n",
    "                unsure_decisions += 1\n",
    "            if not pd.isnull(row[1][2]) or not pd.isnull(row[1][3]) or not pd.isnull(row[1][4]):\n",
    "                spatial_decisions += 1\n",
    "                if row[1][1] == 'Nein, es war ein Gesamteindruck.' or row[1][1] == 'Ich bin mir nicht sicher.':\n",
    "                    wrong_labelled_decisions += 1\n",
    "            else:\n",
    "                if row[1][1] == 'Ja, ich gebe ihn unten an.':\n",
    "                    wrong_labelled_decisions += 1\n",
    "                \n",
    "decisions, spatial_decisions, spatial_decisions / decisions, general_decision, unsure_decisions, wrong_labelled_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def create_heatmap(df, img_number, image_path, group, selection_type):\n",
    "    img = mpimg.imread(image_path)\n",
    "    counts = np.zeros((3, 3))\n",
    "    valid_decisions = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if not row.isna().all():\n",
    "            valid_decisions += 1\n",
    "            for row_idx in range(3):\n",
    "                column_value = row[row_idx]\n",
    "                if pd.isna(column_value):\n",
    "                    continue\n",
    "                if ',' in column_value:\n",
    "                    selections = column_value.split(', ')\n",
    "                else:\n",
    "                    selections = [column_value]\n",
    "                for selection in selections:\n",
    "                    col_idx = ord(selection) - ord('A')\n",
    "                    counts[row_idx, col_idx] += 1\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n",
    "    ax.imshow(img, extent=[0, 640, 480, 0])\n",
    "    cmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", \"red\"])\n",
    "    cax = ax.imshow(counts, interpolation='nearest', cmap=cmap, extent=[0, 640, 480, 0], alpha=0.5)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_title(f'Heatmap für Bild {img_number + 1}')\n",
    "    ax.set_xticks(np.linspace(640 // 6, 640 - 640 // 6, 3))\n",
    "    ax.set_yticks(np.linspace(480 // 6, 480 - 480 // 6, 3))\n",
    "    ax.set_xticklabels(['A', 'B', 'C'])\n",
    "    ax.set_yticklabels(['1', '2', '3'])\n",
    "    ax.set_xlim(0, 640)\n",
    "    ax.set_ylim(480, 0)\n",
    "    plt.figtext(0.5, 0.01, f'Anzahl der Entscheidungen: {valid_decisions}', ha='center', fontsize=12)\n",
    "    plt.savefig(f'statistics/imgs/heatmaps-{group}/{selection_type}heatmap_{img_number + 1}.jpg')\n",
    "    plt.show()\n",
    "\n",
    "for i, img in enumerate(false_negatives_ai):\n",
    "    img_index = int(''.join(filter(str.isdigit, img.columns[0]))) - 1\n",
    "    img = img.iloc[:, 2:5] \n",
    "    create_heatmap(img, img_index, img_paths_1[img_index], 'g1', 'false-negatives-ai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
